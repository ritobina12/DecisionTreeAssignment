{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-012\n",
        "\n",
        "Decision Tree | Assignment\n",
        "\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer: A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, its goal is to predict the class label of a given data point.\n",
        "It works by splitting the dataset into subsets based on the value of input features. This process is repeated recursively, forming a tree-like structure. The main components of a decision tree are:\n",
        "Root Node: The topmost node representing the entire dataset.\n",
        "Internal Nodes: Nodes that represent a decision (or test) on a specific feature, splitting the data into branches.\n",
        "Leaf Nodes: The terminal nodes that represent the final class label or outcome.\n",
        "The algorithm selects the feature that best separates the data into classes (e.g., using Information Gain or Gini Impurity) at each node. It starts from the root, asks a question about a feature, and follows the branch corresponding to the answer. This process continues until it reaches a leaf node, which provides the final classification.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer: Gini Impurity and Entropy are metrics used to measure the \"impurity\" or \"disorder\" of a dataset. A node is \"pure\" (Gini=0, Entropy=0) if all its data points belong to a single class. The goal of a Decision Tree is to create splits that maximize the purity of the resulting child nodes.\n",
        "Gini Impurity: Measures the probability of incorrectly classifying a randomly chosen element from the node if it were randomly labeled according to the class distribution in the node. It is calculated as: G i n i\n",
        "1 − ∑ i\n",
        "1 C ( p i ) 2 Gini=1−∑ i=1 C  (p i  ) 2\n",
        "where p i p i  is the probability of an object being classified to class i i.\n",
        "Entropy: Measures the average amount of \"information\" or \"surprise\" inherent in the node's possible outcomes. A higher entropy means more disorder. It is calculated as: E n t r o p y\n",
        "− ∑ i\n",
        "1 C p i ∗ log ⁡ 2 ( p i ) Entropy=−∑ i=1 C  p i  ∗log 2  (p i  )\n",
        "Impact on Splits: The Decision Tree algorithm evaluates all possible splits on all features. For each potential split, it calculates the weighted average impurity (Gini or Entropy) of the resulting child nodes. The split that results in the largest reduction in impurity (i.e., the greatest increase in purity) is chosen. This reduction is formally called Information Gain when using Entropy. Both metrics lead to similar trees, but Gini is slightly faster to compute, while Entropy might produce more balanced trees.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer: Pruning is a technique used to prevent overfitting in Decision Trees by removing parts of the tree that provide little predictive power.\n",
        "Pre-Pruning (Early Stopping): This involves stopping the tree-building process before it perfectly classifies the training data. This is done by setting constraints (hyperparameters) like max_depth, min_samples_split, and min_samples_leaf.\n",
        "Practical Advantage: It is computationally more efficient because it avoids building an overly complex tree in the first place.\n",
        "Post-Pruning: This involves allowing the tree to fully grow (even overfitting the training data) and then removing non-critical branches or nodes afterward. A common method is Cost Complexity Pruning (CCP).\n",
        "Practical Advantage: It often results in a more accurate and robust tree because it doesn't rely on hard stop criteria during the building phase and can make more nuanced decisions about which branches to cut.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer: Information Gain (IG) is the measure of the effectiveness of a feature in classifying the data. It quantifies the reduction in entropy (or Gini impurity) after a dataset is split on a feature.\n",
        "It is calculated as: I G ( S , A )\n",
        "E n t r o p y ( S ) − ∑ v ∈ V a l u e s ( A ) ∣ S v ∣ ∣ S ∣ E n t r o p y ( S v ) IG(S,A)=Entropy(S)−∑ v∈Values(A)\n",
        "∣S∣ ∣S v  ∣  Entropy(S v  ) Where:\n",
        " S is the original dataset.\n",
        " A is the feature to split on.\n",
        " Values(A) are the possible values of feature A .\n",
        "S v  is the subset of S  where feature  A has value v.\n",
        "Importance: Information Gain is critically important because it provides a quantifiable criterion for selecting the best split at each node. The algorithm computes the Information Gain for every possible split on every feature and then chooses the split with the highest Information Gain. This greedy approach ensures that the most informative features (those that create the purest child nodes) are used higher up in the tree, leading to a more efficient and effective model.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        " Answer: Common Real-World Applications:\n",
        "Finance: Credit scoring and loan approval.\n",
        "Healthcare: Medical diagnosis (e.g., identifying diseases from symptoms).\n",
        "Marketing: Customer segmentation and predicting churn.\n",
        "Manufacturing: Quality control and fault detection.\n",
        "Main Advantages:\n",
        "Interpretability: The model is white-box and easy to understand and visualize, even for non-experts.\n",
        "Little Data Preprocessing: Requires little data scaling or normalization.\n",
        "Handles Mixed Data: Can work with both numerical and categorical data.\n",
        "Main Limitations:\n",
        "Overfitting: They can easily overfit the training data if not pruned properly, capturing noise as patterns.\n",
        "Instability: Small changes in the data can lead to the generation of a completely different tree.\n",
        "Bias: They can be biased towards features with more levels and may create biased trees if some classes are dominant.\n",
        "\n",
        "Question 6: Write a Python program to load the Iris Dataset, train a Decision Tree Classifier using the Gini criterion, and print the model's accuracy and feature importances.\n",
        "\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "Pk0dyZ78lbzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Feature matrix\n",
        "y = iris.target  # Target vector\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"  {feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odlxkNTSmbbT",
        "outputId": "c3b3d987-edca-4851-96cc-3e39d308ec3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0167\n",
            "  petal length (cm): 0.9061\n",
            "  petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to load the Iris Dataset, train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "FR3I1b1jm6wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train a Decision Tree with max_depth=3\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# 2. Train a fully-grown Decision Tree (no restrictions)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the comparison\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Pruned Tree (max_depth=3): {accuracy_pruned:.4f}\")\n",
        "print(f\"Fully-Grown Tree: {accuracy_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMfrdJfvm2QZ",
        "outputId": "ebcef195-0f37-4bf5-98c2-6f9ff13657c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Pruned Tree (max_depth=3): 1.0000\n",
            "Fully-Grown Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to load the Boston Housing Dataset, train a Decision Tree Regressor, and print the Mean Squared Error (MSE) and feature importances.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "95z0eFwgnM7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing Dataset from an alternative source\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('medv', axis=1)\n",
        "y = data['medv']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(X.columns, dt_regressor.feature_importances_):\n",
        "    print(f\"  {feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQHlXob5nFHU",
        "outputId": "055ae68c-8b58-488c-8192-60294a4644ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.4161\n",
            "\n",
            "Feature Importances:\n",
            "  crim: 0.0513\n",
            "  zn: 0.0034\n",
            "  indus: 0.0058\n",
            "  chas: 0.0000\n",
            "  nox: 0.0271\n",
            "  rm: 0.6003\n",
            "  age: 0.0136\n",
            "  dis: 0.0707\n",
            "  rad: 0.0019\n",
            "  tax: 0.0125\n",
            "  ptratio: 0.0110\n",
            "  b: 0.0090\n",
            "  lstat: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to load the Iris Dataset, tune the Decision Tree's max_depth and min_samples_split using GridSearchCV, and print the best parameters and the resulting model accuracy.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "71GUbekGnU_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],  # None means no limit\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_best = best_dt_model.predict(X_test)\n",
        "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Best Model Accuracy on Test Set: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL1psvs6nRIS",
        "outputId": "6484096e-8d1d-4f6b-c472-200aa427249f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Model Accuracy on Test Set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "mQDVF27snekC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Process:\n",
        "\n",
        "Handle Missing Values:\n",
        "\n",
        "Numerical Features: Impute missing values with the mean, median, or mode of the column. For a more sophisticated approach, use model-based imputation (e.g., K-Nearest Neighbors).\n",
        "\n",
        "Categorical Features: Impute with the mode (most frequent category) or create a new category like \"Unknown.\"\n",
        "\n",
        "Encode Categorical Features:\n",
        "\n",
        "Ordinal Features: Use Label Encoding if the categories have a natural order (e.g., \"low,\" \"medium,\" \"high\").\n",
        "\n",
        "Nominal Features: Use One-Hot Encoding for categories without a natural order (e.g., \"city A,\" \"city B,\" \"city C\").\n",
        "\n",
        "Train a Decision Tree Model:\n",
        "\n",
        "Split the preprocessed data into training and testing sets (e.g., 80-20 split).\n",
        "\n",
        "Initialize a DecisionTreeClassifier.\n",
        "\n",
        "Train the model on the training data using the fit method.\n",
        "\n",
        "Tune its Hyperparameters:\n",
        "\n",
        "Use techniques like GridSearchCV or RandomizedSearchCV to find the optimal combination of hyperparameters.\n",
        "\n",
        "Key parameters to tune include max_depth, min_samples_split, min_samples_leaf, and criterion (gini or entropy). This step is crucial to prevent overfitting and improve generalization.\n",
        "\n",
        "Evaluate its Performance:\n",
        "\n",
        "Use the held-out test set to make predictions with the tuned model.\n",
        "\n",
        "Evaluate using metrics appropriate for classification:\n",
        "\n",
        "Accuracy: Overall correctness.\n",
        "\n",
        "Precision & Recall: Especially important if the disease is rare (class imbalance).\n",
        "\n",
        "F1-Score: Harmonic mean of precision and recall.\n",
        "\n",
        "ROC-AUC Score: Measures the model's ability to distinguish between classes.\n",
        "\n",
        "Analyze the confusion matrix for detailed insight.\n",
        "\n",
        "Business Value:\n",
        "This predictive model could provide immense business value by enabling early and accurate disease detection. This allows healthcare providers to:\n",
        "\n",
        "Intervene Sooner: Initiate treatment plans earlier, potentially leading to better patient outcomes and lower treatment costs.\n",
        "\n",
        "Optimize Resources: Efficiently allocate medical resources (like specialist time and diagnostic tests) to high-risk patients.\n",
        "\n",
        "Develop Proactive Care: Shift from a reactive to a proactive healthcare model, focusing on prevention and early intervention, thereby improving overall population health and reducing long-term healthcare expenditures."
      ],
      "metadata": {
        "id": "wJtgR6iUnlLd"
      }
    }
  ]
}